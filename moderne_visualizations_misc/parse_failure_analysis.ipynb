{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parse failures"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This report shows different stack traces from the projects organized into clusters based on similarities. Each cluster represents a group of related themes. The legend provides a representative sample from each cluster to help understand the overall theme within the stack traces.\n",
        "_Note_: The representation of the stack traces are being projected into 2D for visualization purposes only, which is why some clusters might look spread out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "from moderne_pkg.clustering_src.constants import SEPARATOR_TOKEN, CLS_TOKEN\n",
        "from moderne_pkg.clustering_src.utils import pool_and_normalize\n",
        "from moderne_pkg.clustering_src.datasets_loader import prepare_tokenizer\n",
        "from moderne_pkg.clustering_src.preprocessing_utils import truncate_sentences\n",
        "from abc import ABC, abstractmethod\n",
        "import warnings\n",
        "import moderne_pkg.helpers as helpers\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['figure.dpi'] = 600\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "args = helpers.get_notebook_args_from_env()\n",
        "\n",
        "# read data table file\n",
        "file_name = args[\"filteredDataTableFileName\"]\n",
        "df = pd.read_csv(file_name, on_bad_lines='skip', skip_blank_lines=True)\n",
        "\n",
        "# Let's clean the data to prevent mixed types in stackTrace column\n",
        "df.dropna(subset=['stackTrace'])\n",
        "df['stackTrace'] = df['stackTrace'].astype(str)\n",
        "\n",
        "# Exit early if there are no stack traces and render a plot with a message\n",
        "if len(df) == 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.text(0.5, 0.5, 'No stack traces found', ha='center', va='center', fontsize=16, bbox=dict(facecolor='lightgray', edgecolor='black'))\n",
        "    plt.show()\n",
        "    \n",
        "# We need more than one stack trace\n",
        "elif len(df) == 1:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.text(0.5, 0.5, f'Only 1 stack trace found:\\n{df[\"stackTrace\"][0][:80]}', ha='center', va='center', fontsize=16, bbox=dict(facecolor='lightgray', edgecolor='black'))\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    #encoder models\n",
        "    DEVICE = \"cpu\"\n",
        "    # set max input length dynamically\n",
        "    MAX_INPUT_LEN = int(df[\"stackTrace\"].str.len().quantile(0.9))\n",
        "\n",
        "    # max token is between 10 and 512, and is a percentage of the max input length\n",
        "    MAX_TOKEN_LEN = max(min(int((MAX_INPUT_LEN / (len(df)) * 100)), 512), 10)\n",
        "\n",
        "\n",
        "    def set_device(inputs: Dict[str, torch.Tensor], device: str) -> Dict[str, torch.Tensor]:\n",
        "        output_data = {}\n",
        "        for k, v in inputs.items():\n",
        "            output_data[k] = v.to(device)\n",
        "        \n",
        "        return output_data\n",
        "\n",
        "    class BaseEncoder(torch.nn.Module, ABC):\n",
        "\n",
        "        def __init__(self, device, max_input_len, maximum_token_len, model_name):\n",
        "            super().__init__()\n",
        "\n",
        "            self.model_name = model_name\n",
        "            self.tokenizer = prepare_tokenizer(model_name)\n",
        "            self.encoder = AutoModel.from_pretrained(model_name).to(DEVICE).eval()\n",
        "            self.device = device\n",
        "            self.max_input_len = max_input_len\n",
        "            self.maximum_token_len = maximum_token_len\n",
        "        \n",
        "            \n",
        "        \n",
        "        @abstractmethod\n",
        "        def forward(self,):\n",
        "            pass\n",
        "        \n",
        "        def encode(self, input_sentences, batch_size=64, **kwargs):\n",
        "\n",
        "            truncated_input_sentences = truncate_sentences(input_sentences, self.max_input_len)\n",
        "\n",
        "            n_batches = len(truncated_input_sentences) // batch_size + int(len(truncated_input_sentences) % batch_size > 0)\n",
        "\n",
        "            embedding_batch_list = []\n",
        "\n",
        "            for i in range(n_batches):\n",
        "                start_idx = i*batch_size\n",
        "                end_idx = min((i+1)*batch_size, len(truncated_input_sentences))\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    embedding_batch_list.append(\n",
        "                        self.forward(truncated_input_sentences[start_idx:end_idx]).detach().cpu()\n",
        "                    )\n",
        "\n",
        "            input_sentences_embedding = torch.empty(0)  # Initialize an empty tensor\n",
        "            \n",
        "            if len(embedding_batch_list) >= 1:\n",
        "                # ensure a non-empty list of Tensors \n",
        "                input_sentences_embedding = torch.cat(embedding_batch_list)\n",
        "\n",
        "            return [emb.squeeze().numpy() for emb in input_sentences_embedding]\n",
        "\n",
        "    class BigCodeEncoder(BaseEncoder):\n",
        "\n",
        "        def __init__(self, device, max_input_len, maximum_token_len):\n",
        "            super().__init__(device, max_input_len, maximum_token_len, model_name = \"moderne_pkg/starencoder\")\n",
        "        \n",
        "        def forward(self, input_sentences):\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                [f\"{CLS_TOKEN}{sentence}{SEPARATOR_TOKEN}\" for sentence in input_sentences], \n",
        "                padding=\"longest\",\n",
        "                max_length=self.maximum_token_len,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "                )\n",
        "\n",
        "            outputs = self.encoder(**set_device(inputs, self.device))\n",
        "            embedding = pool_and_normalize(outputs.hidden_states[-1], inputs.attention_mask)\n",
        "\n",
        "            return embedding\n",
        "\n",
        "    bigcode_model = BigCodeEncoder(DEVICE, MAX_INPUT_LEN, MAX_TOKEN_LEN)\n",
        "\n",
        "    # Apply the function to a stackTrace and create new column with the results\n",
        "    df['embeddings'] = bigcode_model.encode(list(df['stackTrace']))\n",
        "    embds = df[\"embeddings\"]\n",
        "    stacktraces = df[\"stackTrace\"]\n",
        "    embds = np.array([embd for embd in embds])\n",
        "\n",
        "    perplexity = max(min(30, len(embds) - 1), 0)\n",
        "\n",
        "    two_d_visualization = np.array(TSNE(n_components=2, random_state=0, n_iter=250, perplexity=perplexity).fit_transform(np.array(embds)), dtype=\"float\")\n",
        "\n",
        "    #find best k \n",
        "    sil = []\n",
        "    potential_k = [2,3,5,7,8,11]\n",
        "\n",
        "    # dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n",
        "    for k in potential_k:\n",
        "        if len(embds) >= k:\n",
        "            kmeans = KMeans(n_clusters = k, random_state=0, n_init=\"auto\", algorithm=\"elkan\").fit(embds)\n",
        "            labels = kmeans.labels_\n",
        "\n",
        "            if len(np.unique(labels)) > 2:\n",
        "                sample_size = int(len(embds)*0.2) if len(embds) >= 100 else None\n",
        "                sil.append(silhouette_score(embds, labels, metric='euclidean', sample_size=sample_size))\n",
        "            else:\n",
        "                sil.append(0)\n",
        "        else:\n",
        "            sil.append(0)\n",
        "\n",
        "    best_k = potential_k[sil.index(max(sil))]\n",
        "\n",
        "    kmeans = np.array(KMeans(n_clusters=best_k, random_state=0, n_init=\"auto\", algorithm=\"elkan\").fit_predict(embds), dtype=\"float\")\n",
        "\n",
        "    x = two_d_visualization[:,0]\n",
        "    y = two_d_visualization[:,1]\n",
        "\n",
        "\n",
        "    # Creating a colormap with unique colors for each label\n",
        "    num_labels = len(set(kmeans))\n",
        "    colors = helpers.get_moderne_qualitative_palette(500)\n",
        "    marker_styles = ['o', 's', 'D', '^', 'x', '*', '+', '.']\n",
        "\n",
        "\n",
        "    # Plotting the data with unique colors\n",
        "    scatter_plots = []\n",
        "    for i in range(num_labels):\n",
        "        mask = np.array(kmeans) == i\n",
        "        scatter_plot = plt.scatter(np.array(x)[mask], np.array(y)[mask], c=colors[i%5], marker=marker_styles[i%8], label=f'{i}')\n",
        "        scatter_plots.append(scatter_plot)\n",
        "\n",
        "    # Create a dictionary to store the first cluster label and sample stack trace for each cluster\n",
        "    cluster_labels = {}\n",
        "    for cluster, cluster_label in enumerate(range(num_labels)):\n",
        "        mask = kmeans == cluster\n",
        "        if np.any(mask):\n",
        "            sample_stack_trace = df['stackTrace'][mask].iloc[0].split('\\n')[0][:80]\n",
        "            cluster_labels[cluster] = (cluster_label, sample_stack_trace)\n",
        "\n",
        "    handles = []\n",
        "    labels = []\n",
        "\n",
        "    # Create the legend handles and labels\n",
        "    for cluster in range(num_labels):\n",
        "        count = df['stackTrace'][kmeans == cluster_labels[cluster][0]].describe()[\"count\"]\n",
        "        handles.append(scatter_plots[cluster_labels[cluster][0]])\n",
        "        labels.append(f'{cluster_labels[cluster][1]} (count: {count})')\n",
        "\n",
        "    # Create the legend outside of the plot\n",
        "    plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(-.1, -.1),fontsize='small', title=\"Sample from cluster\")\n",
        "\n",
        "    # Displaying the plot\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "orig_nbformat": 4
}
