{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse failures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report shows different stack traces from the projects organized into clusters based on similarities. Each cluster represents a group of related themes. The legend provides a representative sample from each cluster to help understand the overall theme within the stack traces.\n",
    "_Note_: The representation of the stack traces are being projected into 2D for visualization purposes only, which is why some clusters might look spread out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "from moderne_pkg.clustering_src.constants import SEPARATOR_TOKEN, CLS_TOKEN\n",
    "from moderne_pkg.clustering_src.utils import pool_and_normalize\n",
    "from moderne_pkg.clustering_src.datasets_loader import prepare_tokenizer\n",
    "from moderne_pkg.clustering_src.preprocessing_utils import truncate_sentences\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "import moderne_pkg.helpers as helpers\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"figure.dpi\"] = 600\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "args = helpers.get_notebook_args_from_env()\n",
    "\n",
    "# read data table file\n",
    "file_name = args[\"filteredDataTableFileName\"]\n",
    "df = pd.read_csv(file_name, on_bad_lines=\"skip\", skip_blank_lines=True)\n",
    "\n",
    "# Let's clean the data to prevent mixed types in stackTrace column\n",
    "df.dropna(subset=[\"stackTrace\"])\n",
    "df[\"stackTrace\"] = df[\"stackTrace\"].astype(str)\n",
    "\n",
    "# Exit early if there are no stack traces and render a plot with a message\n",
    "if len(df) == 0:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        \"No stack traces found\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=16,\n",
    "        bbox=dict(facecolor=\"lightgray\", edgecolor=\"black\"),\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# We need more than one stack trace\n",
    "elif len(df) == 1:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        f\"Only 1 stack trace found:\\n{df['stackTrace'][0][:80]}\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=16,\n",
    "        bbox=dict(facecolor=\"lightgray\", edgecolor=\"black\"),\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    # encoder models\n",
    "    DEVICE = \"cpu\"\n",
    "    # set max input length dynamically\n",
    "    MAX_INPUT_LEN = int(df[\"stackTrace\"].str.len().quantile(0.9))\n",
    "\n",
    "    # max token is between 10 and 512, and is a percentage of the max input length\n",
    "    MAX_TOKEN_LEN = max(min(int((MAX_INPUT_LEN / (len(df)) * 100)), 512), 10)\n",
    "\n",
    "    def set_device(\n",
    "        inputs: Dict[str, torch.Tensor], device: str\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        output_data = {}\n",
    "        for k, v in inputs.items():\n",
    "            output_data[k] = v.to(device)\n",
    "\n",
    "        return output_data\n",
    "\n",
    "    class BaseEncoder(torch.nn.Module, ABC):\n",
    "        def __init__(self, device, max_input_len, maximum_token_len, model_name):\n",
    "            super().__init__()\n",
    "\n",
    "            self.model_name = model_name\n",
    "            self.tokenizer = prepare_tokenizer(model_name)\n",
    "            self.encoder = AutoModel.from_pretrained(model_name).to(DEVICE).eval()\n",
    "            self.device = device\n",
    "            self.max_input_len = max_input_len\n",
    "            self.maximum_token_len = maximum_token_len\n",
    "\n",
    "        @abstractmethod\n",
    "        def forward(\n",
    "            self,\n",
    "        ):\n",
    "            pass\n",
    "\n",
    "        def encode(self, input_sentences, batch_size=64, **kwargs):\n",
    "            truncated_input_sentences = truncate_sentences(\n",
    "                input_sentences, self.max_input_len\n",
    "            )\n",
    "\n",
    "            n_batches = len(truncated_input_sentences) // batch_size + int(\n",
    "                len(truncated_input_sentences) % batch_size > 0\n",
    "            )\n",
    "\n",
    "            embedding_batch_list = []\n",
    "\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, len(truncated_input_sentences))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    embedding_batch_list.append(\n",
    "                        self.forward(truncated_input_sentences[start_idx:end_idx])\n",
    "                        .detach()\n",
    "                        .cpu()\n",
    "                    )\n",
    "\n",
    "            input_sentences_embedding = torch.empty(0)  # Initialize an empty tensor\n",
    "\n",
    "            if len(embedding_batch_list) >= 1:\n",
    "                # ensure a non-empty list of Tensors\n",
    "                input_sentences_embedding = torch.cat(embedding_batch_list)\n",
    "\n",
    "            return [emb.squeeze().numpy() for emb in input_sentences_embedding]\n",
    "\n",
    "    class BigCodeEncoder(BaseEncoder):\n",
    "        def __init__(self, device, max_input_len, maximum_token_len):\n",
    "            super().__init__(\n",
    "                device,\n",
    "                max_input_len,\n",
    "                maximum_token_len,\n",
    "                model_name=\"moderne_pkg/starencoder\",\n",
    "            )\n",
    "\n",
    "        def forward(self, input_sentences):\n",
    "            inputs = self.tokenizer(\n",
    "                [\n",
    "                    f\"{CLS_TOKEN}{sentence}{SEPARATOR_TOKEN}\"\n",
    "                    for sentence in input_sentences\n",
    "                ],\n",
    "                padding=\"longest\",\n",
    "                max_length=self.maximum_token_len,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            outputs = self.encoder(**set_device(inputs, self.device))\n",
    "            embedding = pool_and_normalize(\n",
    "                outputs.hidden_states[-1], inputs.attention_mask\n",
    "            )\n",
    "\n",
    "            return embedding\n",
    "\n",
    "    bigcode_model = BigCodeEncoder(DEVICE, MAX_INPUT_LEN, MAX_TOKEN_LEN)\n",
    "\n",
    "    # Apply the function to a stackTrace and create new column with the results\n",
    "    df[\"embeddings\"] = bigcode_model.encode(list(df[\"stackTrace\"]))\n",
    "    embdz = df[\"embeddings\"]\n",
    "    stacktraces = df[\"stackTrace\"]\n",
    "    embds = np.array([embd for embd in embdz])\n",
    "\n",
    "    perplexity = max(min(30, len(embds) - 1), 0)\n",
    "\n",
    "    two_d_visualization = np.array(\n",
    "        TSNE(\n",
    "            n_components=2, random_state=0, n_iter=250, perplexity=perplexity\n",
    "        ).fit_transform(np.array(embds)),\n",
    "        dtype=\"float\",\n",
    "    )\n",
    "\n",
    "    # find best k\n",
    "    sil = []\n",
    "    potential_k = [2, 3, 5, 7, 8, 11]\n",
    "\n",
    "    # dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n",
    "    for k in potential_k:\n",
    "        if len(embds) >= k:\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=k, random_state=0, n_init=\"auto\", algorithm=\"elkan\"\n",
    "            ).fit(embds)\n",
    "            labels = kmeans.labels_\n",
    "\n",
    "            if len(np.unique(labels)) > 2:\n",
    "                sample_size = int(len(embds) * 0.2) if len(embds) >= 100 else None\n",
    "                sil.append(\n",
    "                    silhouette_score(\n",
    "                        embds, labels, metric=\"euclidean\", sample_size=sample_size\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                sil.append(0)\n",
    "        else:\n",
    "            sil.append(0)\n",
    "\n",
    "    best_k = potential_k[sil.index(max(sil))]\n",
    "\n",
    "    kmeans = np.array(\n",
    "        KMeans(\n",
    "            n_clusters=best_k, random_state=0, n_init=\"auto\", algorithm=\"elkan\"\n",
    "        ).fit_predict(embds),\n",
    "        dtype=\"float\",\n",
    "    )\n",
    "\n",
    "    x = two_d_visualization[:, 0]\n",
    "    y = two_d_visualization[:, 1]\n",
    "\n",
    "    # Creating a colormap with unique colors for each label\n",
    "    num_labels = len(set(kmeans))\n",
    "    colors = helpers.get_moderne_qualitative_palette(500)\n",
    "    marker_styles = [\"o\", \"s\", \"D\", \"^\", \"x\", \"*\", \"+\", \".\"]\n",
    "\n",
    "    # Plotting the data with unique colors\n",
    "    scatter_plots = []\n",
    "    for i in range(num_labels):\n",
    "        mask = np.array(kmeans) == i\n",
    "        scatter_plot = plt.scatter(\n",
    "            np.array(x)[mask],\n",
    "            np.array(y)[mask],\n",
    "            c=colors[i % 5],\n",
    "            marker=marker_styles[i % 8],\n",
    "            label=f\"{i}\",\n",
    "        )\n",
    "        scatter_plots.append(scatter_plot)\n",
    "\n",
    "    # Create a dictionary to store the first cluster label and sample stack trace for each cluster\n",
    "    cluster_labels = {}\n",
    "    for cluster, cluster_label in enumerate(range(num_labels)):\n",
    "        mask = kmeans == cluster\n",
    "        if np.any(mask):\n",
    "            sample_stack_trace = df[\"stackTrace\"][mask].iloc[0].split(\"\\n\")[0][:80]\n",
    "            cluster_labels[cluster] = (cluster_label, sample_stack_trace)\n",
    "\n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    # Create the legend handles and labels\n",
    "    for cluster in range(num_labels):\n",
    "        count = df[\"stackTrace\"][kmeans == cluster_labels[cluster][0]].describe()[\n",
    "            \"count\"\n",
    "        ]\n",
    "        handles.append(scatter_plots[cluster_labels[cluster][0]])\n",
    "        labels.append(f\"{cluster_labels[cluster][1]} (count: {count})\")\n",
    "\n",
    "    # Create the legend outside of the plot\n",
    "    plt.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(-0.1, -0.1),\n",
    "        fontsize=\"small\",\n",
    "        title=\"Sample from cluster\",\n",
    "    )\n",
    "\n",
    "    # Displaying the plot\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "orig_nbformat": 4
}
